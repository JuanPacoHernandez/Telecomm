# Week 7 Final Project


## PROBLEM DESCRIPTION

The main idea of this project is to build an end-to-end data pipeline, according to the Problem statement's project it's needed to build a Dashboard with two tiles, so this Dashboard would uncover historical patterns in consumption habits from IoT customers, this Descriptive Analysis describes the consumers in two simple insights, by the frequency of age by gender, and the gender categorical distribution. The dataset is composed of three tables, a CRM table with over 13M of records, and two tables Devices (2.4M records) and Revenues (1.7M records). Since the Descriptive Analysis only requires historical data about customer age and gender, the CRM table would be enough to describe these consumer features. And, due to the origin of the dataset, is possible to perform the Pipeline periodically with no need of real-time analysis, so I decided to perform Batch Processing.

Nevertheless for not despising the importance of the rest of the tables (Devices table and Revenue table), non-used in performing the Data Analysis but useful for future Analyses, stored in the Data Warehouse, it is important to ensure Data Integrity for the three tables within a Workflow Orchestration using Infrastructure as Code, that's why this Project covers the use of Terraform (IaC), Spark (PySpark) for Batch Processing and Transformation of data, along with GCP storage service (as Data Lake) and BigQuery (as Data Warehouse) to finally crafting a Dashboard in Data studio and Streamlit tool as Web App for adding Portability to the Dashboard. For complementing there will use Test to Pipelines Data Ingestions and Transformations

Now, the stakeholders would be:

- Investors interested in Telecom companies.
- Telecomm service providers companies.
- Government officials for sharing reports with citizens.
- Public interest of the community.
- Journalists working on tech articles.
- Telecomm enthusiatics.

Once the problem statement has been described I can go diving into the dataset and I can say that is composed of three tables, a CRM table with information about non-sensible data customers, an IoT devices table with information about the customers' devices, and a Revenue table with customer's revenue generated by weeks.

The data consists in three datasets: 

**CRM ATTRIBUTE DESCRIPTION** 
- msisdn  : Unique identification number assigned to each mobile number
- gender  : sex of the customer using the mobile service
- year_of_birth  : year of birth of the customer
- system_status  : indicates the status of the mobile service being used by the customer
- mobile_type  : Customers can choose their service as prepaid or postpaid
- value_segment  : Segmentation based on how well the customer matches the business goals

**IOT DEVICES ATTRIBUTE DESCRIPTION**
- msisdn: Unique identification number assigned to each mobile number
- imei_tac: Unique identification number assigned to the location of the mobile service
- brand_name: The brand of the mobile
- model_name: The model of the mobile
- os_name: The Operating System of the mobile
- os_vendor: The company of the mobile operating system

**REVENUE ATTRIBUTE DESCRIPTION**
- msisdn  : Unique identification number assigned to each mobile number
- week_number : Week number for the particular year
- Revenue_usd : Revenue generated in that week in US dollars

Dataset sources: 
**https://www.kaggle.com/datasets/krishnacheedella/telecom-iot-crm-dataset?resource=download**

For determining the life expectancy of US citizens up to 2020 year:
**https://data.worldbank.org/indicator/SP.DYN.LE00.IN?locations=US**

PACKAGES THAT NEED TO BE INSTALLED:
- **Prefect**
- **Spark (PySpark)**
- **gcloud**

## CLOUD

## BATCH DATA INGESTION/WORKFLOW ORCHESTRATION WITH PREFECT (PIPELINE FROM PROCESSING AND INGEST INTO DATA LAKE)

When performing Data ingestion, using Batch Processing, this project use PySpark along with Bash to:

- **Extract** three datasets from Kaggle using Kaggle API (Bash commands)
- **Transform** for cleansing stage using PySpark, and 
- **Load** using Bash commands, the datasets to a Data Lake in a Bucket located in Cloud Storage (GCP).

The **webToGCS_Pipeline.py** python3 file is a Pipeline to process the datasets and putting them to a Data Lake in GCP, before running this Pipeline a setup must be enabled:

- ALLOW PERMISSIONS TO BASH SCRIPTS INCLUDED IN THE REPO:
	
	`chmod +x fetching_data.sh uploading_data.sh crm_gcs_to_bq.sh dev_gcs_to_bq.sh rev_gcs_to_bq.sh`
	
- RUN PREFECT ORION SERVER, BUT FIRST ACTIVATE ANACONDA BASE ENVIRONMENT:

	`prefect orion start`

- THEN SET A CONFIGURATION:

	`prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api`

- NEXT SET AUTHORIZATION:

	`gcloud auth login`

- SET PROJECT ID:

	`gcloud config set project <PROJECT ID>`
	
- CREATE A DIR AT YOUR GCS:
	
	`data/telecomm`

THEN RUN PIPELINE:

`python3 webToGCS_Pipeline.py`




## DATA WAREHOUSE


## TRANSFORMATIONS


## DASHBOARD


## REPRODUCIBILITY



################ QUITAR RUTAS DE DATASETS IN BIGQUERY Y EL NOMBRE DEL JSON FILE EN GCSToBQ_pipeline.py ################################


## TESTING STAGE

**https://towardsdatascience.com/how-to-test-pyspark-etl-data-pipeline-1c5a6ab6a04b**


## WEBSITE LINKS:

- EXPORTING GCS TO BIGQUERY:
**https://cloud.google.com/bigquery/docs/external-data-cloud-storage?hl=es-419**

- BIGQUERY CONNECTOR WITH SPARK:
**https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=es-419#pyspark**

- PARTITION BY IN BIGQUERY:
**https://cloud.google.com/bigquery/docs/creating-partitioned-tables?hl=es#sql_2**
**https://towardsdatascience.com/how-to-use-partitions-and-clusters-in-bigquery-using-sql-ccf84c89dd65**

- CLUSTERED BY BIGQUERY:
**https://cloud.google.com/bigquery/docs/creating-clustered-tables?hl=es-419#sql_1**




